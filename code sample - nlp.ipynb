{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sample code is mainly about pre-processing the Twitter data that is collected by calling Twitter API, and it's a class project that I did with guidance from my professor. The code hasn't been run in this notebook but should be error-free and executable. The purpose of this notebook is preparing the tweets for further steps, such as topic modeling and text mining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the libraries and functions needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tweepy.streaming           import StreamListener\n",
    "from tweepy                     import OAuthHandler\n",
    "from tweepy                     import Stream\n",
    "from time                       import strftime\n",
    "from json                       import loads, dumps\n",
    "from pickle                     import dump\n",
    "from time                       import strftime\n",
    "from nltk.corpus                import TwitterCorpusReader\n",
    "from nltk.tokenize              import TweetTokenizer\n",
    "from unicodedata                import category\n",
    "from sklearn.base               import BaseEstimator, TransformerMixin\n",
    "from nltk                       import pos_tag, WordNetLemmatizer\n",
    "from nltk.corpus                import stopwords\n",
    "from nltk.corpus                import wordnet as wn\n",
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the working directory (below is just the directory I used)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grp_dir='/Users/nicoleli/Desktop/textMining/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the API into variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "consumer_key        = u'2k7nmVtEUOhCjsxC3MqQxd2h4'\n",
    "consumer_secret     = u'N3evtiwz5IJNjvxTPL3zPFufy8UE9JXdoVh7KGNUWYidOd5PBD'\n",
    "access_token        = u'994799344674099201-cwx454YZWBC66WOJWkctoq0rbb3arco' \n",
    "access_token_secret = u'ug8CTfeF2AbbBZhtH3nA76P3nMvvmsRHpwxV53R7MbAXx' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authenticate Twitter account\n",
    "The following code calls the api to connect to the designated Twitter account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tweepy import OAuthHandler, API\n",
    "\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the class that reads the streaming data from Twitter and saves the data in files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StdOutListener(StreamListener):\n",
    "  def __init__(self, path=grp_dir, file='tweet_data'):\n",
    "    self.tweet_data = []\n",
    "    self.tweet_count = 0\n",
    "    self.path = path\n",
    "    self.file = file\n",
    "    self.full = self.path+self.file+\"_\"+strftime(\"%Y%m%d%H%M%S\")\n",
    "        \n",
    "  def on_data(self, data): # counts the number of tweetsand index the tweets\n",
    "    self.tweet_count += 1\n",
    "    print(\"on_data\", self.tweet_count)\n",
    "    try:\n",
    "      tweet = loads(data)\n",
    "      print(tweet['text'])\n",
    "    \n",
    "      self.tweet_data.append(tweet)\n",
    "    except Exception as Argument:\n",
    "      print (\"Exception:\\n\", Argument)\n",
    "    return True\n",
    "  \n",
    "  def save_data(self): # saves the tweets to specified paths\n",
    "    print(\"Saving to:\",        self.full+\".pkl\")\n",
    "    dump(self.tweet_data, open(self.full+\".pkl\", \"wb\"))\n",
    "\n",
    "    print(\"Saving to:\",self.full+\".jsonl\")\n",
    "    with          open(self.full+\".jsonl\", 'w') as fd:\n",
    "      for twt in self.tweet_data:\n",
    "        fd.write(dumps(twt)+\"\\n\")\n",
    "  \n",
    "  def on_error(self, status):\n",
    "    print(status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an StdOutListener object called twitter_listener so that the tweets can be saved into files called twitter_file_1.jsonl/.pkl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twitter_listener = StdOutListener(file='twitter_file_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect the tweets that are related to a specified keyword, for example \"chairs\". This step takes time to run, and requires to be stopped manually when enough tweets are collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stream = Stream(auth, twitter_listener)\n",
    "stream.filter(track=['chairs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data just collected into files that are specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twitter_listener.save_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions to filter/clean the tweets, and they are used in the AltTwitterCorpusReader class which is defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def re_filter(tweet,re_list): # the function is only used to remove tokens which are links currently as defined below\n",
    "    r = [re.compile(x) for x in re_list]\n",
    "    for re_comp in r:\n",
    "        tweet=filter(re_comp.match, tweet)\n",
    "    return tweet\n",
    "      \n",
    "def is_punct(token): #True if every character is punctuation\n",
    "    return all(category(char).startswith('P') for char in token)\n",
    "\n",
    "def wnpos(tag): #Return the WordNet POS tag\n",
    "    return {'n': wn.NOUN,\n",
    "            'v': wn.VERB,\n",
    "            'r': wn.ADV,\n",
    "            'j': wn.ADJ\n",
    "           }.get(tag[0].lower(), wn.ADJ_SAT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define AltTwitterCorpusReader class that can clean the tweets, including removing punctuations, urls, stop words and etc., tokenizing, lemmatizing and vectorizing the tweets. The methods implement the steps in vectorizing the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AltTwitterCorpusReader(TwitterCorpusReader):\n",
    "\n",
    "    lemmatize = WordNetLemmatizer().lemmatize\n",
    "     \n",
    "    # assign default parameters    \n",
    "    def __init__(self, tag_list      =['n','v','r','j'],\n",
    "                       base_stopwords=set(stopwords.words('english')),\n",
    "                       more_stopwords=set([]),\n",
    "                       re_list       =['^(?!http)'],\n",
    "                       min_len       =4,\n",
    "                       **kwargs):\n",
    "        TwitterCorpusReader.__init__(self,**kwargs)\n",
    "        self.stopwords = base_stopwords.union(more_stopwords)\n",
    "        self.tag_list  = tag_list\n",
    "        self.re_list   = re_list\n",
    "        self.min_len   = min_len\n",
    "        \n",
    "    def tokenize(self):\n",
    "        self.tokens_   = TwitterCorpusReader.tokenized(self)\n",
    "        return self\n",
    "        \n",
    "    def filter_tokens(self): # filters out the urls, punctuations, words that are shorter than minimum length and stopwords\n",
    "        self.tokens_filtered_regexp_    = [list(re_filter(tweet,self.re_list)) \n",
    "                                           for tweet in self.tokens_]\n",
    "        self.tokens_filtered_punct_     = [[token for token \n",
    "                                            in tweet \n",
    "                                            if not is_punct(token)\n",
    "                                           ] for tweet in self.tokens_filtered_regexp_]\n",
    "        self.tokens_filtered_min_len_   = [[token for token \n",
    "                                            in tweet \n",
    "                                            if len(token)>=self.min_len\n",
    "                                           ] for tweet in self.tokens_filtered_punct_]\n",
    "        self.tokens_filtered_stopwords_ = [[token for token \n",
    "                                            in tweet \n",
    "                                            if token not in self.stopwords\n",
    "                                           ] for tweet in self.tokens_filtered_min_len_]\n",
    "        self.tokens_filtered_           = self.tokens_filtered_stopwords_\n",
    "        return self\n",
    "      \n",
    "    def add_pos(self):\n",
    "        self.tokens_pos_                = [[(token, wnpos(tag)) for (token,tag) \n",
    "                                            in  pos_tag(tweet)\n",
    "                                           ] for tweet in self.tokens_filtered_]\n",
    "        return self\n",
    "      \n",
    "    def filter_pos(self): # only keeps the words of specified forms\n",
    "        self.tokens_pos_filtered_       =  [[(token, tag) for (token,tag) \n",
    "                                             in tweet if  tag in self.tag_list\n",
    "                                            ] for tweet in self.tokens_pos_]\n",
    "        return self\n",
    "        \n",
    "    def normalize(self): # transforms the words back into their original form according to the context\n",
    "        self.tokens_normalized_         = [[(self.lemmatize(token, tag), tag) \n",
    "                                            for (token, tag) in tweet\n",
    "                                           ] for tweet in self.tokens_pos_filtered_]\n",
    "        return self\n",
    "\n",
    "    def vectorize(self): \n",
    "        self.dictionary_                = [{token:1 for (token,tag) in tweet\n",
    "                                           } for tweet in self.tokens_normalized_]\n",
    "        v = DictVectorizer(dtype =np.int16,\n",
    "                           sparse=False)\n",
    "        self.vectorized_ = v.fit_transform(self.dictionary_)\n",
    "        self.vec_vocabulary_ = v.vocabulary_\n",
    "        self.vec_feature_names_ =v.feature_names_\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an AltTwitterCorpusReader object called twitter_reader with specified arguments, which would be the parameters for the functions called to clean the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twitter_reader = AltTwitterCorpusReader(root          =grp_dir,\n",
    "                                       fileids       ='twitter_file_1.jsonl',\n",
    "                                       word_tokenizer=TweetTokenizer(preserve_case=False,strip_handles=True),\n",
    "                                       re_list       =['^(?!http)'],\n",
    "                                       more_stopwords=set([]),\n",
    "                                       min_len       =4,\n",
    "                                       tag_list      =['n','j','v','r']\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the methods in the AltTwitterCorpusReader class step by step, the result from the previous step is stored in the objectand is used in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twitter_reader.tokenize()     \\\n",
    "              .filter_tokens()\\\n",
    "              .add_pos()      \\\n",
    "              .filter_pos()   \\\n",
    "              .normalize()    \\\n",
    "              .vectorize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the first element/item from the result of each step in order to check if the results satisfy the expectation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = 0\n",
    "print('\\ntokens',twitter_reader.tokens_[n])\n",
    "print('\\ntokens filtered',twitter_reader.tokens_filtered_[n])\n",
    "print('\\ntokens with pos',twitter_reader.tokens_pos_[n])\n",
    "print('\\npos filtered',twitter_reader.tokens_pos_filtered_[n])\n",
    "print('\\nnormalized',twitter_reader.tokens_normalized_[n])\n",
    "print('\\ndictionary',twitter_reader.dictionary_[n])\n",
    "print('\\nvectorized',twitter_reader.vectorized_[n])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
